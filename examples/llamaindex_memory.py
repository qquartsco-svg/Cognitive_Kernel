"""
ğŸ”— Cognitive Kernel + LlamaIndex Integration Example

LlamaIndexì˜ ChatMemoryBuffer ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬
Cognitive Kernelì˜ ì¥ê¸° ê¸°ì–µì„ LlamaIndex ì—ì´ì „íŠ¸ì— í†µí•©í•©ë‹ˆë‹¤.

Features:
- Persistent memory across restarts
- PageRank-based importance ranking
- Automatic session management

Usage:
    pip install cognitive-kernel llama-index
    python examples/llamaindex_memory.py
"""

from typing import List, Dict, Any, Optional
from cognitive_kernel import CognitiveKernel

try:
    from llama_index.core.memory import BaseChatMemory, ChatMessage
    from llama_index.core.base.llms.types import ChatMessage, MessageRole
    LLAMAINDEX_AVAILABLE = True
except ImportError:
    LLAMAINDEX_AVAILABLE = False
    BaseChatMemory = None
    ChatMessage = None
    MessageRole = None


if LLAMAINDEX_AVAILABLE:
    class CognitiveKernelMemory(BaseChatMemory):
        """
        LlamaIndex-compatible memory using Cognitive Kernel.
        
        Provides:
        - Persistent storage (survives process restart)
        - PageRank-based importance ranking
        - Automatic decay over time
        """
        
        def __init__(self, session_name: str = "llamaindex_agent", **kwargs):
            super().__init__(**kwargs)
            self.kernel = CognitiveKernel(session_name)
            self.session_name = session_name
        
        def __enter__(self):
            self.kernel.__enter__()
            return self
        
        def __exit__(self, *args):
            self.kernel.__exit__(*args)
        
        def get_all(self) -> List[ChatMessage]:
            """Get all chat messages from memory."""
            memories = self.kernel.recall(k=50)  # Get recent memories
            
            messages = []
            for mem in memories:
                content = mem.get('content', {})
                if isinstance(content, dict):
                    text = content.get('text', str(content))
                else:
                    text = str(content)
                
                event_type = mem.get('event_type', 'message')
                
                # Determine role from event type
                if 'user' in event_type.lower() or 'human' in event_type.lower():
                    role = MessageRole.USER
                elif 'assistant' in event_type.lower() or 'ai' in event_type.lower():
                    role = MessageRole.ASSISTANT
                else:
                    role = MessageRole.USER  # Default
                
                messages.append(ChatMessage(
                    role=role,
                    content=text
                ))
            
            return messages
        
        def get(self, initial_token_count: Optional[int] = None) -> List[ChatMessage]:
            """Get chat messages, optionally limited by token count."""
            all_messages = self.get_all()
            
            if initial_token_count is None:
                return all_messages
            
            # Simple token estimation (rough: 1 token â‰ˆ 4 chars)
            selected = []
            token_count = 0
            
            for msg in reversed(all_messages):  # Start from most recent
                msg_tokens = len(msg.content) // 4
                if token_count + msg_tokens <= initial_token_count:
                    selected.insert(0, msg)
                    token_count += msg_tokens
                else:
                    break
            
            return selected
        
        def put(self, message: ChatMessage) -> None:
            """Store a chat message in memory."""
            event_type = "user_message" if message.role == MessageRole.USER else "ai_response"
            
            self.kernel.remember(
                event_type=event_type,
                content={"text": message.content, "role": message.role.value},
                importance=0.7 if message.role == MessageRole.USER else 0.5
            )
        
        def set(self, messages: List[ChatMessage]) -> None:
            """Replace all messages in memory."""
            # Clear existing memories (create new session)
            self.kernel = CognitiveKernel(f"{self.session_name}_reset")
            
            # Add all messages
            for msg in messages:
                self.put(msg)
        
        def reset(self) -> None:
            """Clear all memories."""
            self.kernel = CognitiveKernel(f"{self.session_name}_reset")


# ============================================================
# ğŸ¯ Demo: LlamaIndex Integration
# ============================================================

def demo_llamaindex_integration():
    """LlamaIndex + Cognitive Kernel í†µí•© ë°ëª¨"""
    
    if not LLAMAINDEX_AVAILABLE:
        print("\nâŒ LlamaIndex not installed")
        print("\nğŸ“¦ Install required packages:")
        print("   pip install llama-index")
        return
    
    print("\n" + "="*60)
    print("ğŸ”— LlamaIndex + Cognitive Kernel Integration")
    print("="*60)
    
    # 1. Cognitive Kernel Memory ì´ˆê¸°í™”
    print("\nğŸ“¦ Step 1: Initialize Cognitive Kernel Memory")
    print("-" * 60)
    
    with CognitiveKernelMemory("llamaindex_demo") as memory:
        print("   âœ… Cognitive Kernel Memory initialized")
        
        # 2. ëŒ€í™” ì €ì¥
        print("\nğŸ’¬ Step 2: Store Conversation")
        print("-" * 60)
        
        messages = [
            ChatMessage(role=MessageRole.USER, content="My name is Alice"),
            ChatMessage(role=MessageRole.ASSISTANT, content="Nice to meet you, Alice!"),
            ChatMessage(role=MessageRole.USER, content="I love hiking and photography"),
            ChatMessage(role=MessageRole.ASSISTANT, content="Those are great hobbies!"),
            ChatMessage(role=MessageRole.USER, content="Remember: I prefer afternoon meetings"),
            ChatMessage(role=MessageRole.ASSISTANT, content="Got it! I'll remember that."),
        ]
        
        for msg in messages:
            memory.put(msg)
            print(f"   âœ… Stored: [{msg.role.value}] {msg.content[:40]}...")
        
        print(f"\n   ğŸ“ Total messages stored: {len(messages)}")
    
    print("\n   ğŸ’¾ Session ended â†’ Auto-saved to disk")
    
    # 3. ì„¸ì…˜ ë³µêµ¬ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ Step 3: Session Recovery Test")
    print("-" * 60)
    
    with CognitiveKernelMemory("llamaindex_demo") as memory:
        recovered = memory.get_all()
        print(f"   âœ… Recovered {len(recovered)} messages from previous session")
        
        print("\n   Recovered messages:")
        for i, msg in enumerate(recovered[-3:], 1):  # Show last 3
            print(f"   {i}. [{msg.role.value}] {msg.content[:50]}...")
        
        # 4. ì¤‘ìš”ë„ ê¸°ë°˜ íšŒìƒ
        print("\nğŸ“Š Step 4: Importance-Based Recall")
        print("-" * 60)
        
        # Cognitive Kernelì˜ recall ì‚¬ìš©
        top_memories = memory.kernel.recall(k=3)
        print(f"\n   Top 3 memories by importance:")
        for i, mem in enumerate(top_memories, 1):
            content = mem.get('content', {})
            if isinstance(content, dict):
                text = content.get('text', str(content))
            else:
                text = str(content)
            print(f"   {i}. [{mem.get('event_type')}] Importance: {mem.get('importance', 0):.3f}")
            print(f"      Text: {text[:50]}...")
    
    print("\n" + "="*60)
    print("âœ… Demo completed!")
    print("="*60)


# ============================================================
# ğŸš€ Full LlamaIndex Integration Example
# ============================================================

def full_llamaindex_example():
    """ì™„ì „í•œ LlamaIndex í†µí•© ì˜ˆì œ ì½”ë“œ"""
    
    example_code = '''
# Full LlamaIndex Integration Code:

from llama_index.core.agent import ReActAgent
from llama_index.llms.openai import OpenAI
from examples.llamaindex_memory import CognitiveKernelMemory

# Initialize with persistent memory
with CognitiveKernelMemory("my_assistant") as memory:
    
    # Create LlamaIndex agent
    llm = OpenAI(model="gpt-4")
    
    # Use Cognitive Kernel as memory backend
    agent = ReActAgent.from_tools(
        tools=[],  # Add your tools here
        llm=llm,
        memory=memory,  # â† Persistent, ranked memory!
        verbose=True
    )
    
    # Chat with persistent memory
    response = agent.chat("Remember: I prefer morning meetings")
    print(response)
    
    # Next day (new process), agent still remembers!
    response = agent.chat("When should we schedule our call?")
    # Agent recalls: "You prefer morning meetings"
    
# Memory automatically saved!
'''
    print("\n" + "="*60)
    print("ğŸ”— Full LlamaIndex Integration Example")
    print("="*60)
    print(example_code)


# ============================================================
# ğŸƒ Main
# ============================================================

if __name__ == "__main__":
    print("\nğŸ§  Cognitive Kernel + LlamaIndex Demo")
    print("â”" * 60)
    
    try:
        demo_llamaindex_integration()
        full_llamaindex_example()
        
        print("\n" + "="*60)
        print("ğŸ“Š Summary")
        print("="*60)
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature              â”‚ Standard â”‚ Cognitive Kernel     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Persistence          â”‚    âŒ    â”‚       âœ…            â”‚
â”‚  Importance Ranking   â”‚    âŒ    â”‚       âœ… (PageRank) â”‚
â”‚  Time Decay           â”‚    âŒ    â”‚       âœ…            â”‚
â”‚  Session Management   â”‚  Manual  â”‚       Automatic     â”‚
â”‚  Storage Backend      â”‚  Memory  â”‚  JSON/SQLite/NPZ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """)
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()

