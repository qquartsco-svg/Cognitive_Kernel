#!/usr/bin/env python3
"""
Cognitive Kernel - Mathematical Model Verification Tests

ê° ì—”ì§„ì˜ ìˆ˜í•™ì  ëª¨ë¸ì´ ì´ë¡ ì  ì˜ˆì¸¡ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦.

Tests:
1. Rescorla-Wagner fear learning dynamics
2. HPA axis cortisol dynamics  
3. Memory decay (Ebbinghaus forgetting curve)
4. Q-Learning convergence
5. Softmax action selection distribution
6. PageRank convergence

Author: GNJz (Qquarts)
Date: 2025-01-29
"""

import sys
import math
from pathlib import Path
import numpy as np

# Path setup
root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(root))

print("=" * 70)
print("ğŸ”¬ COGNITIVE KERNEL - Mathematical Model Verification")
print("=" * 70)


# ============================================================================
# TEST 1: Rescorla-Wagner Model (Amygdala Fear Learning)
# ============================================================================

def test_rescorla_wagner():
    """
    ê²€ì¦: Rescorla-Wagner í•™ìŠµ ê·œì¹™
    
    ìˆ˜ì‹: Î”V = Î± Ã— Î² Ã— (Î» - V)
    
    ì˜ˆì¸¡:
    - VëŠ” Î»ì— ìˆ˜ë ´í•´ì•¼ í•¨
    - í•™ìŠµë¥ ì´ ë†’ì„ìˆ˜ë¡ ë¹ ë¥´ê²Œ ìˆ˜ë ´
    - Vê°€ Î»ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ Î”VëŠ” ê°ì†Œ
    """
    print("\n" + "-" * 70)
    print("ğŸ“ TEST 1: Rescorla-Wagner Model (Fear Learning)")
    print("-" * 70)
    
    def rescorla_wagner_update(V, alpha, beta, lambda_max):
        """Rescorla-Wagner learning rule."""
        delta_V = alpha * beta * (lambda_max - V)
        return V + delta_V, delta_V
    
    # Parameters
    alpha = 0.3      # CS salience
    beta = 0.5       # US learning rate
    lambda_max = 1.0 # Maximum associative strength
    
    # Initial state
    V = 0.0
    
    # Learning trajectory
    V_history = [V]
    delta_history = []
    
    for trial in range(20):
        V, delta = rescorla_wagner_update(V, alpha, beta, lambda_max)
        V_history.append(V)
        delta_history.append(delta)
    
    # Verification
    print(f"\n  Parameters: Î±={alpha}, Î²={beta}, Î»={lambda_max}")
    print(f"  Initial V: {V_history[0]:.4f}")
    print(f"  Final V:   {V_history[-1]:.4f}")
    print(f"  Target Î»:  {lambda_max:.4f}")
    
    # Check 1: V should approach Î» (within 5% after 20 trials)
    convergence = abs(V_history[-1] - lambda_max) < 0.05
    print(f"\n  âœ“ Convergence to Î» (within 5%): {convergence} (diff: {abs(V_history[-1] - lambda_max):.6f})")
    
    # Check 2: Î”V should decrease over trials
    delta_decreasing = all(delta_history[i] >= delta_history[i+1] 
                          for i in range(len(delta_history)-1))
    print(f"  âœ“ Î”V monotonically decreasing: {delta_decreasing}")
    
    # Check 3: Learning curve shape (exponential approach)
    # V(t) = Î» Ã— (1 - exp(-kÃ—t)) approximately
    # After 1 trial: V â‰ˆ Î±Ã—Î²Ã—Î»
    expected_v1 = alpha * beta * lambda_max
    actual_v1 = V_history[1]
    v1_match = abs(expected_v1 - actual_v1) < 0.01
    print(f"  âœ“ First trial prediction: {v1_match} (expected: {expected_v1:.4f}, actual: {actual_v1:.4f})")
    
    # Show learning curve
    print(f"\n  Learning Curve:")
    for i in [0, 1, 5, 10, 15, 19]:
        bar = "â–ˆ" * int(V_history[i] * 40)
        print(f"    Trial {i:2d}: V={V_history[i]:.4f} {bar}")
    
    return convergence and delta_decreasing and v1_match


# ============================================================================
# TEST 2: Exponential Decay (Memory Forgetting)
# ============================================================================

def test_memory_decay():
    """
    ê²€ì¦: Ebbinghaus ë§ê° ê³¡ì„ 
    
    ìˆ˜ì‹: R(t) = exp(-t/S) ë˜ëŠ” R(t) = exp(-Î»Ã—t)
    
    ì˜ˆì¸¡:
    - ë°˜ê°ê¸°ì—ì„œ R = 0.5
    - ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë‹¨ì¡° ê°ì†Œ
    - Î» = ln(2) / half_life
    """
    print("\n" + "-" * 70)
    print("ğŸ“ TEST 2: Memory Decay (Ebbinghaus Forgetting Curve)")
    print("-" * 70)
    
    def memory_retention(t, half_life):
        """Exponential memory decay."""
        lambda_decay = math.log(2) / half_life
        return math.exp(-lambda_decay * t)
    
    # Parameters
    half_life = 24.0  # hours (1 day)
    
    # Test points
    test_times = [0, 12, 24, 48, 72, 168]  # hours
    
    print(f"\n  Half-life: {half_life} hours")
    print(f"\n  Retention over time:")
    
    retentions = []
    for t in test_times:
        R = memory_retention(t, half_life)
        retentions.append(R)
        bar = "â–ˆ" * int(R * 40)
        print(f"    t={t:3d}h: R={R:.4f} {bar}")
    
    # Verification
    # Check 1: At t=0, R should be 1.0
    r_at_0 = memory_retention(0, half_life)
    r0_check = abs(r_at_0 - 1.0) < 0.0001
    print(f"\n  âœ“ R(0) = 1.0: {r0_check} (actual: {r_at_0:.6f})")
    
    # Check 2: At t=half_life, R should be 0.5
    r_at_half = memory_retention(half_life, half_life)
    r_half_check = abs(r_at_half - 0.5) < 0.0001
    print(f"  âœ“ R(half_life) = 0.5: {r_half_check} (actual: {r_at_half:.6f})")
    
    # Check 3: Monotonically decreasing
    mono_decrease = all(retentions[i] >= retentions[i+1] 
                       for i in range(len(retentions)-1))
    print(f"  âœ“ Monotonically decreasing: {mono_decrease}")
    
    # Check 4: At t=2Ã—half_life, R should be 0.25
    r_at_2half = memory_retention(2 * half_life, half_life)
    r_2half_check = abs(r_at_2half - 0.25) < 0.0001
    print(f"  âœ“ R(2Ã—half_life) = 0.25: {r_2half_check} (actual: {r_at_2half:.6f})")
    
    return r0_check and r_half_check and mono_decrease and r_2half_check


# ============================================================================
# TEST 3: Q-Learning Convergence
# ============================================================================

def test_q_learning():
    """
    ê²€ì¦: Q-Learning ìˆ˜ë ´ì„±
    
    ìˆ˜ì‹: Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³ Ã— max(Q(s',a')) - Q(s,a)]
    
    ì˜ˆì¸¡:
    - ì¶©ë¶„í•œ íƒí—˜ìœ¼ë¡œ ìµœì  Qê°’ì— ìˆ˜ë ´
    - ë³´ìƒì´ ë†’ì€ í–‰ë™ì˜ Qê°’ì´ ë†’ì•„ì•¼ í•¨
    """
    print("\n" + "-" * 70)
    print("ğŸ“ TEST 3: Q-Learning Convergence")
    print("-" * 70)
    
    def q_update(Q, state, action, reward, next_state, alpha, gamma):
        """Q-learning update."""
        max_Q_next = max(Q.get(next_state, {}).values()) if Q.get(next_state) else 0
        current_Q = Q.get(state, {}).get(action, 0)
        td_error = reward + gamma * max_Q_next - current_Q
        
        if state not in Q:
            Q[state] = {}
        Q[state][action] = current_Q + alpha * td_error
        return Q, td_error
    
    # Simple environment: 2 states, 2 actions
    # State 0 â†’ Action 'good' â†’ Reward 1.0, stay in state 0
    # State 0 â†’ Action 'bad' â†’ Reward 0.1, stay in state 0
    
    alpha = 0.1
    gamma = 0.9
    
    Q = {}
    
    # Training
    print(f"\n  Parameters: Î±={alpha}, Î³={gamma}")
    print(f"  Environment: 'good' action gives reward 1.0, 'bad' gives 0.1")
    print(f"\n  Training Q-values:")
    
    td_errors = []
    for episode in range(100):
        state = 0
        
        # Simulate choosing actions
        if np.random.random() < 0.5:
            action, reward = 'good', 1.0
        else:
            action, reward = 'bad', 0.1
        
        Q, td = q_update(Q, state, action, reward, state, alpha, gamma)
        td_errors.append(abs(td))
        
        if episode in [0, 10, 50, 99]:
            q_good = Q.get(0, {}).get('good', 0)
            q_bad = Q.get(0, {}).get('bad', 0)
            print(f"    Episode {episode:3d}: Q(good)={q_good:.4f}, Q(bad)={q_bad:.4f}")
    
    # Verification
    q_good_final = Q.get(0, {}).get('good', 0)
    q_bad_final = Q.get(0, {}).get('bad', 0)
    
    # Check 1: Q(good) > Q(bad)
    q_ordering = q_good_final > q_bad_final
    print(f"\n  âœ“ Q(good) > Q(bad): {q_ordering}")
    
    # Check 2: Q values should approach V = r / (1 - Î³) for continuing task
    # For good: V â‰ˆ 1.0 / (1 - 0.9) = 10.0
    # But with 50% exploration, it's mixed
    theoretical_q_good = 1.0 / (1 - gamma)
    print(f"  âœ“ Theoretical Q(good) â‰ˆ {theoretical_q_good:.2f} (actual: {q_good_final:.4f})")
    
    # Check 3: TD errors should decrease on average
    early_td = np.mean(td_errors[:20])
    late_td = np.mean(td_errors[-20:])
    td_decreasing = late_td < early_td * 1.5  # Allow some variance
    print(f"  âœ“ TD errors decreasing trend: {td_decreasing} (early: {early_td:.4f}, late: {late_td:.4f})")
    
    return q_ordering


# ============================================================================
# TEST 4: Softmax Action Selection
# ============================================================================

def test_softmax():
    """
    ê²€ì¦: Softmax í–‰ë™ ì„ íƒ
    
    ìˆ˜ì‹: P(a_i) = exp(Î² Ã— U_i) / Î£_j exp(Î² Ã— U_j)
    
    ì˜ˆì¸¡:
    - í™•ë¥  í•© = 1
    - ë†’ì€ íš¨ìš©ì˜ í–‰ë™ì´ ë” ë†’ì€ í™•ë¥ 
    - Î²ê°€ ë†’ì„ìˆ˜ë¡ ë” ê²°ì •ì 
    """
    print("\n" + "-" * 70)
    print("ğŸ“ TEST 4: Softmax Action Selection")
    print("-" * 70)
    
    def softmax(utilities, temperature):
        """Softmax probability distribution."""
        exp_u = np.exp(np.array(utilities) / temperature)
        return exp_u / np.sum(exp_u)
    
    # Test utilities
    utilities = [1.0, 2.0, 3.0]
    
    print(f"\n  Utilities: {utilities}")
    
    # Test different temperatures
    temperatures = [0.5, 1.0, 2.0, 5.0]
    
    print(f"\n  Temperature effects:")
    for temp in temperatures:
        probs = softmax(utilities, temp)
        bar_high = "â–ˆ" * int(probs[2] * 30)
        print(f"    Î²={temp:.1f}: P(low)={probs[0]:.3f}, P(mid)={probs[1]:.3f}, P(high)={probs[2]:.3f} {bar_high}")
    
    # Verification
    # Check 1: Probabilities sum to 1
    probs_1 = softmax(utilities, 1.0)
    sum_check = abs(sum(probs_1) - 1.0) < 0.0001
    print(f"\n  âœ“ Probabilities sum to 1: {sum_check} (sum: {sum(probs_1):.6f})")
    
    # Check 2: Higher utility â†’ higher probability
    ordering_check = probs_1[0] < probs_1[1] < probs_1[2]
    print(f"  âœ“ P(U=1) < P(U=2) < P(U=3): {ordering_check}")
    
    # Check 3: Lower temperature â†’ more deterministic
    probs_low_temp = softmax(utilities, 0.1)
    probs_high_temp = softmax(utilities, 10.0)
    determinism_check = probs_low_temp[2] > probs_high_temp[2]
    print(f"  âœ“ Lower Î² â†’ more deterministic: {determinism_check}")
    print(f"    (Î²=0.1: P(high)={probs_low_temp[2]:.4f}, Î²=10: P(high)={probs_high_temp[2]:.4f})")
    
    # Check 4: Equal utilities â†’ uniform distribution
    equal_utils = [1.0, 1.0, 1.0]
    probs_equal = softmax(equal_utils, 1.0)
    uniform_check = all(abs(p - 1/3) < 0.0001 for p in probs_equal)
    print(f"  âœ“ Equal utilities â†’ uniform: {uniform_check}")
    
    return sum_check and ordering_check and determinism_check and uniform_check


# ============================================================================
# TEST 5: PageRank Convergence
# ============================================================================

def test_pagerank():
    """
    ê²€ì¦: PageRank ìˆ˜ë ´ì„±
    
    ìˆ˜ì‹: r^(t+1) = Î± Ã— M Ã— r^(t) + (1 - Î±) Ã— v
    
    ì˜ˆì¸¡:
    - ë°˜ë³µìœ¼ë¡œ ìˆ˜ë ´
    - ë­í¬ í•© = 1
    - ë§ì´ ì—°ê²°ëœ ë…¸ë“œê°€ ë†’ì€ ë­í¬
    """
    print("\n" + "-" * 70)
    print("ğŸ“ TEST 5: PageRank Convergence")
    print("-" * 70)
    
    def pagerank(M, v, alpha=0.85, max_iter=100, tol=1e-6):
        """Power iteration PageRank."""
        n = M.shape[0]
        r = np.ones(n) / n
        
        for i in range(max_iter):
            r_new = alpha * (M @ r) + (1 - alpha) * v
            r_new = r_new / r_new.sum()
            
            if np.linalg.norm(r_new - r, 1) < tol:
                return r_new, i + 1
            r = r_new
        
        return r, max_iter
    
    # Test graph: A â†’ B â†’ C â†’ A (cycle), with extra link A â†’ C
    # A should have highest rank (most incoming)
    #
    #   A â†â”€â”€â”
    #   â”‚    â”‚
    #   â–¼    â”‚
    #   B    â”‚
    #   â”‚    â”‚
    #   â–¼    â”‚
    #   C â”€â”€â”€â”˜
    #   â”‚
    #   â””â”€â–¶ A (extra)
    
    # Transition matrix (column-normalized)
    # M[i,j] = probability of going from j to i
    M = np.array([
        [0, 0, 1],      # A gets from C
        [1, 0, 0],      # B gets from A
        [0.5, 1, 0],    # C gets from A(0.5) and B
    ])
    
    # Normalize columns
    M = M / M.sum(axis=0, keepdims=True)
    
    # Uniform personalization
    v = np.ones(3) / 3
    
    r, iterations = pagerank(M, v)
    
    print(f"\n  Graph: A â†’ B â†’ C â†’ A")
    print(f"  Damping: Î± = 0.85")
    print(f"  Converged in {iterations} iterations")
    print(f"\n  PageRank scores:")
    labels = ['A', 'B', 'C']
    for i, label in enumerate(labels):
        bar = "â–ˆ" * int(r[i] * 60)
        print(f"    {label}: {r[i]:.4f} {bar}")
    
    # Verification
    # Check 1: Sum to 1
    sum_check = abs(r.sum() - 1.0) < 0.0001
    print(f"\n  âœ“ Ranks sum to 1: {sum_check} (sum: {r.sum():.6f})")
    
    # Check 2: Converged within reasonable iterations
    converge_check = iterations < 50
    print(f"  âœ“ Converged in <50 iterations: {converge_check}")
    
    # Check 3: All ranks positive
    positive_check = all(r > 0)
    print(f"  âœ“ All ranks positive: {positive_check}")
    
    return sum_check and converge_check and positive_check


# ============================================================================
# TEST 6: HPA Axis Stress Dynamics
# ============================================================================

def test_hpa_axis():
    """
    ê²€ì¦: HPA ì¶• ì½”ë¥´í‹°ì†” ë™ì—­í•™
    
    ìˆ˜ì‹: dC/dt = -kâ‚ Ã— C + kâ‚‚ Ã— S Ã— (1 - C/C_max)
    
    ì˜ˆì¸¡:
    - ìŠ¤íŠ¸ë ˆìŠ¤ ì‹œ ì½”ë¥´í‹°ì†” ìƒìŠ¹
    - ìŠ¤íŠ¸ë ˆìŠ¤ ì¢…ë£Œ í›„ ìì—° ê°ì‡ 
    - ìµœëŒ€ê°’ í¬í™”
    """
    print("\n" + "-" * 70)
    print("ğŸ“ TEST 6: HPA Axis Cortisol Dynamics")
    print("-" * 70)
    
    def hpa_update(C, S, k1, k2, C_max, dt):
        """HPA axis cortisol dynamics."""
        dC = (-k1 * C + k2 * S * (1 - C / C_max)) * dt
        return np.clip(C + dC, 0, C_max)
    
    # Parameters
    k1 = 0.1    # Decay rate
    k2 = 0.3    # Production rate
    C_max = 1.0
    dt = 0.1
    
    # Simulation: Stress on for 20 steps, then off for 30 steps
    C = 0.1  # Baseline cortisol
    C_history = [C]
    
    print(f"\n  Parameters: kâ‚={k1}, kâ‚‚={k2}, C_max={C_max}")
    print(f"  Scenario: Stress ON (0-20), Stress OFF (20-50)")
    
    # Stress phase
    for t in range(20):
        S = 0.8  # High stress
        C = hpa_update(C, S, k1, k2, C_max, dt)
        C_history.append(C)
    
    peak_C = C
    
    # Recovery phase
    for t in range(30):
        S = 0.0  # No stress
        C = hpa_update(C, S, k1, k2, C_max, dt)
        C_history.append(C)
    
    final_C = C
    
    # Print trajectory
    print(f"\n  Cortisol trajectory:")
    for i in [0, 10, 20, 30, 40, 50]:
        bar = "â–ˆ" * int(C_history[i] * 40)
        phase = "STRESS" if i <= 20 else "RECOVERY"
        print(f"    t={i:2d} ({phase:8s}): C={C_history[i]:.4f} {bar}")
    
    # Verification
    # Check 1: Cortisol increased during stress
    stress_increase = C_history[20] > C_history[0]
    print(f"\n  âœ“ Cortisol increased during stress: {stress_increase}")
    print(f"    (Start: {C_history[0]:.4f} â†’ Peak: {peak_C:.4f})")
    
    # Check 2: Cortisol decreased during recovery
    recovery_decrease = C_history[-1] < C_history[20]
    print(f"  âœ“ Cortisol decreased during recovery: {recovery_decrease}")
    print(f"    (Peak: {peak_C:.4f} â†’ Final: {final_C:.4f})")
    
    # Check 3: Cortisol stayed below maximum
    below_max = all(c <= C_max for c in C_history)
    print(f"  âœ“ Cortisol stayed â‰¤ C_max: {below_max}")
    
    # Check 4: Final cortisol approaching baseline (with decay)
    approaching_baseline = C_history[-1] < C_history[20] * 0.5
    print(f"  âœ“ Approaching baseline: {approaching_baseline}")
    
    return stress_increase and recovery_decrease and below_max


# ============================================================================
# MAIN
# ============================================================================

def main():
    results = {}
    
    results['rescorla_wagner'] = test_rescorla_wagner()
    results['memory_decay'] = test_memory_decay()
    results['q_learning'] = test_q_learning()
    results['softmax'] = test_softmax()
    results['pagerank'] = test_pagerank()
    results['hpa_axis'] = test_hpa_axis()
    
    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“Š VERIFICATION SUMMARY")
    print("=" * 70)
    
    all_passed = True
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {test_name:20s}: {status}")
        if not passed:
            all_passed = False
    
    print("-" * 70)
    if all_passed:
        print("ğŸ‰ ALL MATHEMATICAL MODELS VERIFIED!")
    else:
        print("âš ï¸ SOME TESTS FAILED - REVIEW IMPLEMENTATIONS")
    
    return all_passed


if __name__ == "__main__":
    main()

